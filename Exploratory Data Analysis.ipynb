{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39c5664-8353-4a01-b3d1-8ec3a1836fd2",
   "metadata": {},
   "source": [
    "# Assignment Email\n",
    "\n",
    "This email was received from David Meltzer on July 6, 2023 at 8:59AM PST. I began work at 6:00PM that day.\n",
    "\n",
    "> Hi Russell,\n",
    ">\n",
    "> As I mentioned last week, the challenge is the last step in our interview process.  I have provided the instructions below, but if you need any clarification or have problems with how we have defined this, please let us know.  To the extent you incur any AWS costs as part of this, we will reimburse you.  There is no fixed deadline for this, but we are hoping to have it back within the next week or so to be able to make a decision.\n",
    ">\n",
    "> ----\n",
    ">\n",
    "> Use the IOT intrusion detection dataset for a supervised anomaly detection task. The label column indicates whether each row's data is normal or anomalous.\n",
    "You can choose an ML classifier for training on this dataset.\n",
    ">\n",
    "> The task is to build a classifier using the train dataset and deploy a trained model in Sagemaker.\n",
    "You can use your personal AWS account to do this.\n",
    "The classifier should be able to generate robust classification metrics on a held-out test/validation dataset. You can use a portion of your train set for validation metrics.\n",
    ">\n",
    "> As for output, please send us a link to the code and classification results in your git repository when you are done. Configuration should be through an orchestration system so we can re-create the environment programmatically.\n",
    ">\n",
    "> We will use a held-out test set to evaluate your model's performance.\n",
    ">\n",
    "> Test data can be found here: [https://github.com/netography/ml-engineer/archive/refs/heads/main.zip](https://github.com/netography/ml-engineer/archive/refs/heads/main.zip)\n",
    ">\n",
    "> It can also be found here: [https://www.dropbox.com/scl/fi/oz1fspqu4mago3wxeb9dp/IoT-network-intrustion-dataset-train.csv?rlkey=d6hblxlw4t163tt386w16gldi&dl=0](https://www.dropbox.com/scl/fi/oz1fspqu4mago3wxeb9dp/IoT-network-intrustion-dataset-train.csv?rlkey=d6hblxlw4t163tt386w16gldi&dl=0)\n",
    ">\n",
    "> -Dave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1060be30-311b-4c9a-815c-28bf190b877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914e601-b4fb-4c41-94d6-b4f5aee5d9ee",
   "metadata": {},
   "source": [
    "# Load and Evaluate the Flow Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56441379-9c11-4b54-9345-9017a6a809c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"data/ml-engineer-main/iot_network_intrustion_dataset_train.parquet\")\n",
    "train_df = train_df.rename(columns={\"Unnamed: 0\": \"ID\"})\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9949bf-84fb-47d5-8ad9-9a327db78897",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc03bb0-8390-4a7f-a3bb-1ab627b7599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(train_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034bdc2-c839-44fb-a3bc-283d70e20a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet(\"data/ml-engineer-main/iot_network_intrustion_dataset_test.parquet\")\n",
    "test_df = test_df.rename(columns={\"Unnamed: 0\": \"ID\"})\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819541c-02d7-4d6b-a30c-2a7e8b89f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b86461-7f72-4796-94fe-83d3e1cc52ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223d1b7-bfe4-4ac6-9947-949f1f0067e3",
   "metadata": {},
   "source": [
    "## How are flows unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd3da3-1cad-40c9-8504-de083c7f6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(train_df[\"ID\"].unique()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27911c38-1361-42d7-8939-a6cd7c09ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(train_df[\"Flow_ID\"].unique()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e280c0b-0a1d-418b-97bc-8336737f6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(train_df[train_df.Flow_ID == \"163.152.127.193-192.168.0.13-10101-56361-17\"]):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259705c-2147-45eb-b82b-1e3e5c958c4e",
   "metadata": {},
   "source": [
    "## This seems like it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe92e91-0a1b-44f4-8823-2373124a701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.Flow_ID == \"163.152.127.193-192.168.0.13-10101-56361-17\"].sort_values([\"Timestamp\", \"ID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd42e88-43fc-4ced-90e7-d08a22e6ac49",
   "metadata": {},
   "source": [
    "## Label Check\n",
    "\n",
    "As we see below, the data is just 6.4% / 6.1% normal event types. I would like to ask a question about this, but at this late date I will go with what I can determine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e3eff-c1cf-4997-9318-eaa1c062cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_counts = train_df[\"Label\"].value_counts()\n",
    "\n",
    "train_normal_pct = (train_label_counts[1] / train_label_counts.sum()) * 100\n",
    "print(f\"Normal label percentage: {train_normal_pct:,.2f}%\\n\")\n",
    "\n",
    "train_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34fe90-c187-475f-b163-72c0d14f3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_counts = test_df[\"Label\"].value_counts()\n",
    "\n",
    "test_normal_pct = (test_label_counts[1] / test_label_counts.sum()) * 100\n",
    "print(f\"Normal label percentage: {test_normal_pct:,.2f}%\\n\")\n",
    "\n",
    "test_label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0cdd4-1c9c-48d4-b65f-d2b20fd7c4a5",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "We might assume the proportion isn't representative of the problem which means we might perform some sample that oversamples normal data. While I don't know how the data was sampled, I am going to assume it is a representative, random sample... it is the only prior I have evidence for at this time. The problem is thus flipped on its head. We need to detect normal traffic and throw out anomalies.\n",
    "\n",
    "Perhaps this is a web server? Let's do some more EDA to understand it, so I can feel comfortable about proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3233156d-6b28-4979-9d51-15a07ab7baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    len(train_df[\"Src_IP\"].unique()),\n",
    "    len(train_df[\"Dst_IP\"].unique()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81f5da-b403-485e-83cb-d40cb08734b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many IPs have more than 1 connection?\n",
    "# ip_counts = train_df.groupby(\"Dst_IP\").filter(lambda x: len(x) > 1)[\"Dst_IP\"].value_counts()\n",
    "\n",
    "ip_counts = train_df[\"Dst_IP\"].value_counts()\n",
    "ip_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9750e9-1f4f-48a0-bc94-713ccbaedbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for a histogram!\n",
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(data=ip_counts, bins=60, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7608af1-1011-45da-91a1-e5f858300373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sense of the distribution across destination IPs\n",
    "dst_label_counts = train_df[[\"Dst_IP\", \"Label\"]].value_counts().reset_index().sort_values(by=[\"Dst_IP\", \"Label\"], ascending=[True, False])\n",
    "dlc_df = dst_label_counts[dst_label_counts[\"Dst_IP\"].str.startswith(\"192.168.0\")]\n",
    "dlc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee3a59-7e15-44d4-9021-3edd72994460",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_counts = train_df[[\"Dst_IP\", \"Label\"]].value_counts().sort_index()\n",
    "ip_counts = ip_counts.to_frame().rename(columns={0: \"count\"})\n",
    "ip_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61680180-3b20-4208-924b-996fe135bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_val_counts = ip_counts.reset_index().rename(columns={\"count\": \"IP_Count\"})\n",
    "ip_val_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5533f74-745a-4fa0-81b8-90e5b267c0a4",
   "metadata": {},
   "source": [
    "### Comparing Anomaly / Label Rations per IP\n",
    "\n",
    "The following histogram is interesting... there is no clear pattern visible in terms of label per destination IP. I am going to stop my EDA here and move on to a baseline unsupervised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078050e-dcf2-4408-b734-d401c8fa60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=ip_val_counts, x=\"IP_Count\", hue=\"Label\", kde=True, bins=50, log_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56602747-0d2c-472b-98d3-adabc50a2446",
   "metadata": {},
   "source": [
    "## Network Visualization\n",
    "\n",
    "I just have to look at this as a network before moving onto actual machine learning... as I just worked at Graphistry and am familiar with their tool.\n",
    "\n",
    "### Preparing a Dataset to Visualize\n",
    "\n",
    "I need to see the network before proceeding onwards because I tend to make fundamental errors when I don't perform this step... sort of like in this exercise - the work I did in CloudFormation to setup a new SageMaker notebook inside a VPC - when I could have just used a SageMaker domain and the SageMaker Python SDK to publish a model very easily. In any case... let's do a first pass visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990b4a4-dc16-430c-8223-d646a2e6d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df = train_df[[\"Flow_ID\", \"Src_IP\", \"Src_Port\", \"Dst_IP\", \"Dst_Port\", \"Timestamp\", \"Protocol\", \"Label\"]]\n",
    "viz_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f14bc3-a3d3-4a02-8910-ce89f1ee808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df[\"src\"] = viz_df[\"Src_IP\"] + \" / \" + viz_df[\"Src_Port\"].astype(\"str\") + \" / \" + viz_df[\"Protocol\"].astype(\"str\")\n",
    "viz_df[\"dst\"] = viz_df[\"Dst_IP\"] + \" / \" + viz_df[\"Dst_Port\"].astype(\"str\") + \" / \" + viz_df[\"Protocol\"].astype(\"str\")\n",
    "\n",
    "viz_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af24132-3ea1-4dfa-b299-132572edce5c",
   "metadata": {},
   "source": [
    "### Are Flow IDs Labeled All or Nothing?\n",
    "\n",
    "As we will see below - yes they are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc8eb5-359e-478c-9d6b-00a30c82ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_label_counts = viz_df.groupby([\"Flow_ID\", \"Label\"]).count().reset_index()\n",
    "flow_label_counts = flow_label_counts.rename(columns={\"Src_IP\": \"Count\"})[[\"Flow_ID\", \"Label\", \"Count\"]]\n",
    "flow_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc851a-8696-4edc-b188-30750fb28e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_anomaly_counts = flow_label_counts[flow_label_counts[\"Label\"] == \"Anomaly\"]\n",
    "flow_anomaly_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a84c1-8053-42e5-a9a3-659500367afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_normal_counts = flow_label_counts[flow_label_counts[\"Label\"] == \"Normal\"]\n",
    "flow_normal_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea9d2c-3fa6-41ae-9f88-c184414829d6",
   "metadata": {},
   "source": [
    "### Look... a flow ID is all normal or all anomalous, regardless of timestamp :)\n",
    "\n",
    "There is no overlap between these two datasets. This simplifies visualizing them considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8d4d2-97d3-43d3-a4b5-391b4a55b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_anomaly_ratio = flow_normal_counts[\"Count\"] / flow_anomaly_counts[\"Count\"]\n",
    "flow_anomaly_ratio.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92781bba-4195-41d8-9d9d-0f669a22f65d",
   "metadata": {},
   "source": [
    "### Alter `viz_df` to Account for Label Polarity\n",
    "\n",
    "Let's dedupe the flows to account for the fact that Flow IDs are always anomalous or not..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51e55f-0e8a-49b4-beab-f9ae98c4b6b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viz_df = viz_df.drop(columns=[\"Timestamp\"], errors=\"ignore\").drop_duplicates()\n",
    "print(f\"Total edges: {len(viz_df):,}\")\n",
    "viz_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa7db2-d23b-40f4-af05-d8c939c166fe",
   "metadata": {},
   "source": [
    "### Setting up Graphistry\n",
    "\n",
    "I used the GPUs freely available on [Graphistry Hub](https://hub.graphistry.com/) at [https://hub.graphistry.com/](https://hub.graphistry.com/) to visualize the flow logs as a network. It is free for personal use and is powerful for visualizing networks large and small.\n",
    "\n",
    "You can [signup](https://hub.graphistry.com/accounts/signup/) for a Graphistry account at [https://hub.graphistry.com/accounts/signup/](https://hub.graphistry.com/accounts/signup/). <b>You should use a username/password/email to get the required credentials</b>, although after that you can login with your Github or Google account.\n",
    "\n",
    "<center><img src=\"images/graphistry_hub_registration.png\" /></center>\n",
    "\n",
    "Retain and use your credentials in the login form and in the environment variables in the next cell below. You should set the `GRAPHISTRY_USERNAME` and `GRAPHISTRY_PASSWORD` variables in the `env/graphistry.env` file, and then restart this docker container to pickup the new values.\n",
    "\n",
    "<center><img src=\"images/graphistry_hub_homepage.png\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0e390-fba2-48ab-a393-19684fc81a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import graphistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501cf64-02db-40eb-b320-536e3d7878e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variable setup\n",
    "GRAPHISTRY_USERNAME = os.getenv(\"GRAPHISTRY_USERNAME\")\n",
    "GRAPHISTRY_PASSWORD = os.getenv(\"GRAPHISTRY_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c5cda-b275-41cf-83f3-c9a68a8c914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphistry.register(\n",
    "    api=3,\n",
    "    username=GRAPHISTRY_USERNAME,\n",
    "    password=GRAPHISTRY_PASSWORD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a388a2-4951-489b-aaaf-4ef1b5b5dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    graphistry.edges(viz_df, source=\"src\", destination=\"dst\")\n",
    "    #.options({\"\"})\n",
    "    .plot()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c66939-c2b4-4b12-af44-6e1571e3aff9",
   "metadata": {},
   "source": [
    "### 192.168.0.13: Real Time Streaming Protocol (RTSP) for Microsoft Windows Media\n",
    "\n",
    "One interesting thing that pops up immediately is the following image of Graphistry visualizing the flow network, which shows port scanning from a number of hosts of the machine \n",
    "\n",
    "<center><img alt=\"192.168.0.13 is an interesting high degree node. Is this a port scan or normal traffic?\" src=\"images/Netography-Network-Flows-Graphistry-Server.jpg\" /></center>\n",
    "<br />\n",
    "\n",
    "You can see below that the flows are mostly around the host `192.168.0.13` serving a [Real Time Streaming Protocol (RTSP) for Microsoft Windows Media streaming services and QuickTime Streaming Server (QTSS)](https://www.speedguide.net/port.php?port=554) workload. You can imagine if we were manually feature engineering that a low port (<1024) with a high degree would likely be legitimate.\n",
    "\n",
    "<br />\n",
    "<center><img alt=\"192.168.0.13 is a Windows Media Server\" src=\"images/Netography-Network-Flows-Graphistry-Windows-Media-Server.jpg\" /></center>\n",
    "<br />\n",
    "\n",
    "\n",
    "\n",
    "Wait... traffic to this server is marked as `Anomaly`. This is very confusing. Things seem reversed once again. I really need to know more about the use case to understand this better... what constitutes normal traffic on this network isn't what I would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b6aa34-69f7-4d17-b5dd-29d2684e1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "thin_df = train_df[train_df.Dst_IP == \"192.168.0.13\"][[\"Flow_ID\", \"Src_IP\", \"Src_Port\", \"Dst_IP\", \"Dst_Port\", \"Protocol\", \"Timestamp\", \"Label\", \"Cat\", \"Sub_Cat\"]]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f8259-4875-43a9-885e-1875e82cc1f3",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "While we could use something like sentence encoding on our IP addresses and the like, for simplicity let's matricize our features - which are mostly numeric - in the simplest, most direct manner possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ca01a-11a9-4150-9757-70464bc45ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675505e-f0e5-49c9-807c-c315757fee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump IDs - this must work on new data. Ignore errors for repeats.\n",
    "train_df = train_df.drop(columns=[\"ID\", \"Flow_ID\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc04ba7-95c4-4f80-a48d-379ad89583bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b24b85-2d71-4eb4-a01e-7357b5c9429f",
   "metadata": {},
   "source": [
    "### String Columns\n",
    "\n",
    "I am going to ordinal encode the string columns. `Cat` and `Sub_Cat` in particular look useful. [Ordinal encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) in scikit-learn over label encoding allows for new values to be encoded once the model is deployed.\n",
    "\n",
    "#### Timestamp...\n",
    "\n",
    "Not sure what to do with timestamp... it should be relative, but to what? Probably a difference between it and the previous flow log.\n",
    "\n",
    "Leaving it out on a first pass. I will window function a diff from the last value if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39e6b0-0cef-47fc-b94c-e6c99b4affbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_cols = list(train_df.columns[train_df.dtypes == \"object\"].values)\n",
    "str_cols_no_dt = [x for x in str_cols if x != \"Timestamp\"]\n",
    "str_cols_no_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59734d1-5b9c-44a8-b3f9-b7ed2de203ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_train_df = train_df[str_cols_no_dt]\n",
    "str_train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645121c-c2ea-4815-9e5c-12c1af78536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow, Cat is informative\n",
    "str_train_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d8526-02fc-4246-bbb9-081888beeefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Encode unknown values as -1 as this is for production - retrain to pickup the new flow IPs, etc.\n",
    "ordinal_encoder = OrdinalEncoder(\n",
    "    categories=\"auto\",\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f99a9a-3bed-40e2-838d-b56901573ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.fit(str_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a70daa6-f5dc-45b5-bf27-082e6b086c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_str_train = ordinal_encoder.transform(str_train_df)\n",
    "X_str_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109a70d-8b5e-4ba7-bbc4-70bd7f2ddaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we are dropping the timestamp at this point - potentially a big deal\n",
    "numeric_train_df = train_df.drop(columns=str_cols, errors=\"ignore\")\n",
    "\n",
    "# We have np.inf as values - not good for KMeans below\n",
    "numeric_train_df = numeric_train_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "X_numeric_train = numeric_train_df.values\n",
    "X_numeric_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67f8fb-14d5-46b7-9163-b38c3ec7ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.append(X_numeric_train, X_str_train, axis=1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc50fc-46f6-455d-8418-52293fdfbad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ran into a problem below, now we np.nan impute infinities above\n",
    "np.isinf(X_train).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810fac5-90af-4489-9cc3-160dd89d8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With infinities gone, NaNs are problematic too... so let's impute the column average to eliminate signal\n",
    "feature_means = np.nanmean(X_train,axis=0)\n",
    "feature_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b5f07-d3c4-4096-bf89-6c5b6dde6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find indices that you need to replace\n",
    "nan_indices = np.where(np.isnan(X_train))\n",
    "\n",
    "# np.inf and np.nans appear in these columns\n",
    "numeric_train_df.columns[16:18]\n",
    "\n",
    "numeric_train_df[\"Flow_Byts/s\"].isnull().sum(), numeric_train_df[\"Flow_Pkts/s\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44461ffd-3835-4592-9291-0983e26f9b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place column means in the indices. Align the arrays using take\n",
    "X_train[nan_indices] = np.take(feature_means, nan_indices[1])\n",
    "\n",
    "# Should now be zero infs and nans\n",
    "np.sum(np.where(np.isnan(X_train))), np.sum(np.where(np.isinf(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1d03d-0cd3-4e23-9328-40cd2e31aec8",
   "metadata": {},
   "source": [
    "## Statistical Anomaly Detection with KMeans in `scikit-learn`\n",
    "\n",
    "Pulled this as a first baseline from a KMeans example from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab573ec-eb58-42b8-bfbb-8e563462ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit the k-means algorithm to the dataset\n",
    "kmeans = KMeans(n_clusters=2).fit(X_train)\n",
    "\n",
    "# Get the distances of each point to its nearest cluster\n",
    "distances = kmeans.transform(X_train)\n",
    "nearest_distances = np.min(distances, axis=1)\n",
    "\n",
    "# Define a threshold for anomaly detection\n",
    "threshold = np.percentile(nearest_distances, 95)\n",
    "\n",
    "# Identify anomaly indices\n",
    "anomalies = np.where(nearest_distances > threshold)\n",
    "\n",
    "# Print the indices of the anomalies\n",
    "print(\"Anomalies:\", anomalies)\n",
    "anomalies[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8b12c-8c56-4faa-9c8b-a2093deb20c2",
   "metadata": {},
   "source": [
    "## Scoring Anomaly Detection Algorithms\n",
    "\n",
    "We need a consistent way to score the anomaly detection methods. Given the label imbalance where normal traffic is only 6.4% of the training data and 6.1% of the tet data, accuracy isn't going to work very well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4d15a-b4ad-428b-9792-c815c4e7997d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a9b71-06b0-455e-9a8d-649ef7112dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53889f-ca33-47a8-bbcd-11074d516473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abb0c5dc-170a-451b-937f-0f712906e863",
   "metadata": {},
   "source": [
    "## PyOD\n",
    "\n",
    "PyOD is the leading anomaly detection library..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34cc57-ed3c-42fe-94ff-58302f7477e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from pyod.models.knn import KNN \n",
    "from pyod.utils.data import generate_data, get_outliers_inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d751f-4cff-423e-a897-1034df8ca4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
